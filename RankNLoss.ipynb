{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282e5494-e73b-4dca-b84c-e245d736be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import keras\n",
    "import keras.models\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, GlobalAveragePooling2D, Dropout, Reshape\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchaudio.transforms as T\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2160468e-3f59-43b0-8941-9a4434b9154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 108, 108]           3,136\n",
      "       BatchNorm2d-2         [-1, 64, 108, 108]             128\n",
      "              ReLU-3         [-1, 64, 108, 108]               0\n",
      "         MaxPool2d-4           [-1, 64, 54, 54]               0\n",
      "            Conv2d-5           [-1, 64, 54, 54]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 54, 54]             128\n",
      "              ReLU-7           [-1, 64, 54, 54]               0\n",
      "            Conv2d-8           [-1, 64, 54, 54]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 54, 54]             128\n",
      "             ReLU-10           [-1, 64, 54, 54]               0\n",
      "       BasicBlock-11           [-1, 64, 54, 54]               0\n",
      "           Conv2d-12           [-1, 64, 54, 54]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 54, 54]             128\n",
      "             ReLU-14           [-1, 64, 54, 54]               0\n",
      "           Conv2d-15           [-1, 64, 54, 54]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 54, 54]             128\n",
      "             ReLU-17           [-1, 64, 54, 54]               0\n",
      "       BasicBlock-18           [-1, 64, 54, 54]               0\n",
      "           Conv2d-19          [-1, 128, 27, 27]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 27, 27]             256\n",
      "             ReLU-21          [-1, 128, 27, 27]               0\n",
      "           Conv2d-22          [-1, 128, 27, 27]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 27, 27]             256\n",
      "           Conv2d-24          [-1, 128, 27, 27]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 27, 27]             256\n",
      "             ReLU-26          [-1, 128, 27, 27]               0\n",
      "       BasicBlock-27          [-1, 128, 27, 27]               0\n",
      "           Conv2d-28          [-1, 128, 27, 27]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 27, 27]             256\n",
      "             ReLU-30          [-1, 128, 27, 27]               0\n",
      "           Conv2d-31          [-1, 128, 27, 27]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 27, 27]             256\n",
      "             ReLU-33          [-1, 128, 27, 27]               0\n",
      "       BasicBlock-34          [-1, 128, 27, 27]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "        AvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           ResNet-68                  [-1, 512]               0\n",
      "================================================================\n",
      "Total params: 11,170,240\n",
      "Trainable params: 11,170,240\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.18\n",
      "Forward/backward pass size (MB): 59.03\n",
      "Params size (MB): 42.61\n",
      "Estimated Total Size (MB): 101.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, base_width=64):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, base_width=64):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        width = int(planes * (base_width / 64.0))\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, in_channel=1, dropout=None, width_per_group=64):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=7, stride=2, padding=3, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) \n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "\n",
    "        self.use_dropout = True if dropout else False\n",
    "        if self.use_dropout:\n",
    "            print(f'Using dropout: {dropout}')\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.base_width))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, base_width=self.base_width))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "model_dict = {\n",
    "    'resnet18': [resnet18, 512]\n",
    "}\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, name='resnet18'):\n",
    "        super(Encoder, self).__init__()\n",
    "        model_fun, dim_in = model_dict[name]\n",
    "        self.encoder = model_fun()\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        return feat\n",
    "\n",
    "model = Encoder()\n",
    "model.cuda()\n",
    "summary(model, (1,216,216))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b48eb53-fa25-45e1-b31e-79c7a02e47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "spectrograms_array = np.load('spectrograms.npy')\n",
    "labels_array = np.load('labels.npy')\n",
    "\n",
    "spectrograms_array = spectrograms_array/80 + 1\n",
    "spectrograms_array = np.expand_dims(spectrograms_array, axis=1)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(spectrograms_array), torch.from_numpy(labels_array)\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  \n",
    "val_size = int(0.1 * len(dataset))    \n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    "    \n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1217e5-62b1-4a8f-a21a-06c0435dc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(x1, x2):\n",
    "    \n",
    "    x1 = x1[x1 !=0 ]\n",
    "    x2 = x2[x2 !=0 ]\n",
    "    \n",
    "    x1_instruments = x1[0::3]\n",
    "    x2_instruments = x2[0::3]\n",
    "\n",
    "    x1_pitches = x1[2::3]\n",
    "    x2_pitches = x2[2::3]\n",
    "\n",
    "    ipx1 = set([(x1[i], x1[i+2]) for i in range(0, len(x1), 3)])\n",
    "    ipx2 = set([(x2[i], x2[i+2]) for i in range(0, len(x2), 3)])\n",
    "\n",
    "    shared_dist = len(ipx1.intersection(ipx2)) / len(ipx1.union(ipx2))\n",
    "    \n",
    "\n",
    "    instrument_dist = np.intersect1d(x1_instruments, x2_instruments).size / np.union1d(x1_instruments, x2_instruments).size\n",
    "    pitches_dist = np.intersect1d(x1_pitches, x2_pitches).size / np.union1d(x1_pitches, x2_pitches).size\n",
    "    \n",
    "    \n",
    "    return 1 - (0.5*pitches_dist + 0.5*instrument_dist + 0*shared_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865b0947-6839-4911-94fc-3010c9fd4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDifference(nn.Module):\n",
    "    def __init__(self, distance_type='jaccard'):\n",
    "        super(LabelDifference, self).__init__()\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "    def forward(self, labels):\n",
    "        #labels: [bs, label_dim]\n",
    "        #output: [bs, bs]\n",
    "\n",
    "        labels = labels.cpu()\n",
    "        x = labels.shape[0]\n",
    "\n",
    "        matrix = np.zeros((x,x))\n",
    "\n",
    "        for i in range(x):\n",
    "            for j in range(x):\n",
    "                matrix[i][j] = jaccard_distance(labels[i],labels[j])\n",
    "\n",
    "        return torch.from_numpy(matrix).to('cuda')\n",
    "\n",
    "class FeatureSimilarity(nn.Module):\n",
    "    def __init__(self, similarity_type='l2'):\n",
    "        super(FeatureSimilarity, self).__init__()\n",
    "        self.similarity_type = similarity_type\n",
    "\n",
    "    def forward(self, features):\n",
    "        # labels: [bs, feat_dim]\n",
    "        # output: [bs, bs]\n",
    "        if self.similarity_type == 'l2':\n",
    "            return -(features[:, None, :] - features[None, :, :]).norm(2, dim=-1)\n",
    "        else:\n",
    "            raise ValueError(self.similarity_type)\n",
    "\n",
    "\n",
    "class RnCLoss(nn.Module):\n",
    "    def __init__(self, temperature=2, label_diff='jaccard', feature_sim='l2'):\n",
    "        super(RnCLoss, self).__init__()\n",
    "        self.t = temperature\n",
    "        self.label_diff_fn = LabelDifference(label_diff)\n",
    "        self.feature_sim_fn = FeatureSimilarity(feature_sim)\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        # features: [bs, 2, feat_dim]\n",
    "        # labels: [bs, label_dim]\n",
    "\n",
    "        features = torch.cat([features[:, 0], features[:, 1]], dim=0)  # [2bs, feat_dim]\n",
    "        labels = labels.repeat(2, 1)  # [2bs, label_dim]\n",
    "\n",
    "        label_diffs = self.label_diff_fn(labels)\n",
    "        logits = self.feature_sim_fn(features).div(self.t)\n",
    "        logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n",
    "        logits -= logits_max.detach()\n",
    "        exp_logits = logits.exp()\n",
    "\n",
    "        n = logits.shape[0]  # n = 2bs\n",
    "\n",
    "        # remove diagonal\n",
    "        logits = logits.masked_select((1 - torch.eye(n).to(logits.device)).bool()).view(n, n - 1)\n",
    "        exp_logits = exp_logits.masked_select((1 - torch.eye(n).to(logits.device)).bool()).view(n, n - 1)\n",
    "        label_diffs = label_diffs.masked_select((1 - torch.eye(n).to(logits.device)).bool()).view(n, n - 1)\n",
    "\n",
    "        loss = 0.\n",
    "        for k in range(n - 1):\n",
    "            pos_logits = logits[:, k]  # 2bs\n",
    "            pos_label_diffs = label_diffs[:, k]  # 2bs\n",
    "            neg_mask = (label_diffs >= pos_label_diffs.view(-1, 1)).float()  # [2bs, 2bs - 1]\n",
    "            pos_log_probs = pos_logits - torch.log((neg_mask * exp_logits).sum(dim=-1))  # 2bs\n",
    "            loss += - (pos_log_probs / (n * (n - 1))).sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c9352e-746b-4512-9fb1-ba4dd6d16cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5, momentum=0.9, weight_decay=1e-4)\n",
    "loss_fn = RnCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6ba4ee-ae94-4d09-8625-bbdfcfaf77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for 1 batch) at step 0: 5.6259\n",
      "Seen so far: 128 samples\n",
      "vloss:  tensor(6.2773, device='cuda:0')\n",
      "Training loss (for 1 batch) at step 0: 5.3325\n",
      "Seen so far: 128 samples\n",
      "vloss:  tensor(9.4614, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m logits2 \u001b[38;5;241m=\u001b[39m model(inputs2)\n\u001b[1;32m     30\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((logits1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), logits2\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, epoch)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m, in \u001b[0;36mRnCLoss.forward\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m     46\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([features[:, \u001b[38;5;241m0\u001b[39m], features[:, \u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [2bs, feat_dim]\u001b[39;00m\n\u001b[1;32m     47\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [2bs, label_dim]\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m label_diffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_diff_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_sim_fn(features)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n\u001b[1;32m     51\u001b[0m logits_max, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mLabelDifference.forward\u001b[0;34m(self, labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x):\n\u001b[0;32m---> 17\u001b[0m         matrix[i][j] \u001b[38;5;241m=\u001b[39m \u001b[43mjaccard_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(matrix)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mjaccard_distance\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m      9\u001b[0m x1_pitches \u001b[38;5;241m=\u001b[39m x1[\u001b[38;5;241m2\u001b[39m::\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     10\u001b[0m x2_pitches \u001b[38;5;241m=\u001b[39m x2[\u001b[38;5;241m2\u001b[39m::\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m ipx1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([(x1[i], x1[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(x1), \u001b[38;5;241m3\u001b[39m)])\n\u001b[1;32m     13\u001b[0m ipx2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([(x2[i], x2[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(x2), \u001b[38;5;241m3\u001b[39m)])\n\u001b[1;32m     15\u001b[0m shared_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ipx1\u001b[38;5;241m.\u001b[39mintersection(ipx2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(ipx1\u001b[38;5;241m.\u001b[39munion(ipx2))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/_tensor.py:964\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;21m__neg__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_TensorBase\u001b[38;5;241m.\u001b[39mneg\n\u001b[1;32m    962\u001b[0m \u001b[38;5;21m__abs__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_TensorBase\u001b[38;5;241m.\u001b[39mabs\n\u001b[0;32m--> 964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "best_vloss = 1_000_000.\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    model.train(True)\n",
    "    \n",
    "    for step, (inputs, targets) in enumerate(train_dataloader):#\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        #augmentation\n",
    "        time_masking = T.TimeMasking(time_mask_param=0)\n",
    "        freq_masking = T.FrequencyMasking(freq_mask_param=0)\n",
    "\n",
    "        inputs2 = inputs.clone()\n",
    "\n",
    "        time_masked1 = time_masking(inputs)\n",
    "        aug_inputs1 = freq_masking(time_masked1)\n",
    "\n",
    "        time_masked2 = time_masking(inputs2)\n",
    "        aug_inputs2 = freq_masking(time_masked2)\n",
    "        \n",
    "        logits1 = model(inputs)\n",
    "        logits2 = model(inputs2)\n",
    "\n",
    "        features = torch.cat((logits1.unsqueeze(1), logits2.unsqueeze(1)), dim=1)\n",
    "        \n",
    "        loss = loss_fn(features, targets)\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer variable updates\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(\n",
    "                f\"Training loss (for 1 batch) at step {step}: {float(loss):.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
    " \n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs, vlabels = vinputs.cuda(), vlabels.cuda()\n",
    "            voutputs = model(vinputs)\n",
    "            val_features = torch.cat((voutputs.unsqueeze(1), voutputs.unsqueeze(1)), dim=1)\n",
    "            vloss = loss_fn(val_features, vlabels)\n",
    "            running_vloss += vloss\n",
    "            num_batches += 1\n",
    "    \n",
    "\n",
    "    avg_vloss = running_vloss / num_batches\n",
    "    writer.add_scalar(\"Loss/val\", avg_vloss, epoch)\n",
    "\n",
    "    print(\"vloss: \", avg_vloss)\n",
    "    \n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        #torch.save(model.state_dict(), f\"models/epoch_{epoch + 1}_val_loss_{best_vloss:.4f}.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), 'models/final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78a916-d702-405c-97df-a337f0721a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9dfba-5523-492f-8de0-32e7b05a4164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b57b94-66f5-4e80-bbf5-c243af6e0792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a332d78-24a8-40c1-8791-14adac63b1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
